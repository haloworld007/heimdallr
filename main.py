import os
import sys
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
from dotenv import load_dotenv
from app.crew import HeimdallrCrew
from app.llms import llm_manager

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®"""
    openai_key = os.getenv("OPENAI_API_KEY")
    gemini_key = os.getenv("GEMINI_API_KEY")
    
    if not openai_key and not gemini_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°APIå¯†é’¥é…ç½®")
        print("è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶ï¼Œå¹¶é…ç½®ä»¥ä¸‹å˜é‡ä¹‹ä¸€:")
        print("  OPENAI_API_KEY=your_openai_api_key")
        print("  GEMINI_API_KEY=your_gemini_api_key")
        print("\nå‚è€ƒ env.example æ–‡ä»¶æŸ¥çœ‹é…ç½®ç¤ºä¾‹")
        return False
    
    if openai_key:
        print("âœ… æ£€æµ‹åˆ° OpenAI API å¯†é’¥")
    if gemini_key:
        print("âœ… æ£€æµ‹åˆ° Gemini API å¯†é’¥")
    
    return True

# åˆå§‹åŒ– FastAPI åº”ç”¨
app = FastAPI(
    title="Heimdallr: AI å‘Šè­¦ä¸ Issue åˆ†æåŠ©æ‰‹",
    description="ä¸€ä¸ªé€šè¿‡ API æ¥æ”¶å‘Šè­¦å¹¶å¼‚æ­¥è¿”å›åˆ†ææŠ¥å‘Šçš„æ™ºèƒ½ç³»ç»Ÿã€‚",
    version="1.0.0"
)

# å®šä¹‰è¯·æ±‚ä½“æ¨¡å‹
class AnalyzeRequest(BaseModel):
    alert_text: str
    response_url: str

def run_crew_analysis(request: AnalyzeRequest):
    """ç”¨äºåœ¨åå°è¿è¡Œçš„ Crew åˆ†æä»»åŠ¡"""
    try:
        print(f"å¼€å§‹åˆ†æä»»åŠ¡ï¼Œå‘Šè­¦: {request.alert_text}")
        heimdallr_crew = HeimdallrCrew(request.alert_text, request.response_url)
        crew = heimdallr_crew.setup_crew()
        result = crew.kickoff()
        print(f"ä»»åŠ¡å®Œæˆï¼Œæœ€ç»ˆæŠ¥å‘Š: {result}")
    except Exception as e:
        # åœ¨å®é™…ç”Ÿäº§ä¸­ï¼Œè¿™é‡Œåº”è¯¥æœ‰æ›´å¥å£®çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
        print(f"åˆ†æä»»åŠ¡å¤±è´¥: {e}")

@app.post("/analyze")
async def analyze_alert(request: AnalyzeRequest, background_tasks: BackgroundTasks):
    """
    æ¥æ”¶å‘Šè­¦åˆ†æè¯·æ±‚ï¼Œç«‹å³è¿”å›ç¡®è®¤ï¼Œå¹¶åœ¨åå°å¯åŠ¨åˆ†æä»»åŠ¡ã€‚
    """
    # å°†è€—æ—¶çš„åˆ†æä»»åŠ¡æ·»åŠ åˆ°åå°æ‰§è¡Œ
    background_tasks.add_task(run_crew_analysis, request)
    
    # ç«‹å³è¿”å›å“åº”
    return {"message": "Request received. Heimdallr is analyzing the issue..."}

@app.get("/")
async def root():
    return {"message": "Heimdallr is running."}

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "healthy",
        "service": "Heimdallr AI Alert & Issue Analyzer",
        "version": "1.0.0"
    }

@app.get("/config")
async def config_info():
    """è·å–LLMé…ç½®ä¿¡æ¯ï¼Œç”¨äºè°ƒè¯•"""
    try:
        config = llm_manager.get_llm_config_info()
        return {
            "status": "success",
            "config": config
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

# å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œç”¨äºæœ¬åœ°å¼€å‘
if __name__ == "__main__":
    print("ğŸ—ï¸  å¯åŠ¨ Heimdallr AI å‘Šè­¦ä¸ Issue åˆ†æåŠ©æ‰‹...")
    
    # æ£€æŸ¥ç¯å¢ƒé…ç½®
    if not check_environment():
        sys.exit(1)
    
    # å¯åŠ¨æœåŠ¡å™¨
    import uvicorn
    print("ğŸš€ å¯åŠ¨ FastAPI æœåŠ¡å™¨...")
    print("ğŸ“– API æ–‡æ¡£: http://localhost:8000/docs")
    print("ğŸ” å¥åº·æ£€æŸ¥: http://localhost:8000/health")
    print("âš™ï¸  é…ç½®ä¿¡æ¯: http://localhost:8000/config")
    print("\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    
    try:
        uvicorn.run(
            app, 
            host="0.0.0.0", 
            port=int(os.getenv("PORT", "8000")),
            reload=False,  # ç”Ÿäº§ç¯å¢ƒå»ºè®®è®¾ä¸º False
            log_level="info"
        )
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å·²åœæ­¢")
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)